---
title: "Machine Learning in R"
author: "Chenshu Liu"
date: "April 2022"
output:
  pdf_document: default
  html_document: default
---

\section{Packages, message = F}
```{r}
# for data splitting into training and testing (general data manipulation)
library(caTools)

# dataset for logistic regression
# install.packages("mlbench")
library(mlbench)

# Decision tree
# install.packages("FSelector")
# install.packages("caret", dependencies = T)
# install.packages("rpart.plot")
# install.packages("data.tree")
# install.packages("rJava")

# for constructing decision tree
system("java -version")
library(FSelector)
library(rpart)
# for test train set split
library(caret)
library(dplyr)
# plotting the decision tree
library(rpart.plot)
library(data.tree)
```

\section{Linear Regression}
Linear regression is a way to find the best fit linear expression that can show the observed trend(s). The parameters of the best fit line $y = mx + c$ can be calculated by:
$$m = \frac{(n\times \Sigma(x\times y)) - (\Sigma(x)\times \Sigma(y))}{(n\times \Sigma(x^2)) - (\Sigma(x)^2)}$$
$$c = \frac{(\Sigma(y)\times \Sigma(x^2)) - (\Sigma(x)\times \Sigma(x\times y))}{(n\times \Sigma(x^2)) - (\Sigma(x)^2)}$$

\subsection{Data}
```{r, message = FALSE}
sales <- read.csv("/Users/chenshu/Documents/Programming/R/Machine Learning in R/datasets/revenue.csv")

# split into training and testing data
set.seed(2)
# SplitRatio means the percentage of data for training
split <- caTools::sample.split(sales$Profit, SplitRatio = 0.7)
train <- sales[split, ]
test <- sales[!split, ]
```

\subsection{Modeling}
```{r}
Model <- lm(Profit ~., data = train)
summary(Model)
```

\subsection{Predict}
```{r}
pred <- predict(Model, test)

# comparing predicted vs. actual values
plot(test$Profit, type = 'l', lty = 1.8, col = "red")
lines(pred, type = 'l', lty = 1.8, col = "blue")

# determining prediction accuracy
rmse <- sqrt(mean(pred - test$Profit)^2)
rmse
```

\section{Logistic Regression}
Logistic regression is a \textbf{classification algorithm}, not a linear prediction algorithm. Different from linear regression, which is usually used to determine the magnitude of the effect, logistic regression is used to predict binary outcome.
\subsection{Data}
```{r}
data(PimaIndiansDiabetes)
log_df <- PimaIndiansDiabetes
head(log_df)

# splitting data
split <- caTools::sample.split(log_df$diabetes, SplitRatio = 0.7)
train <- log_df[split, ]
test <- log_df[!split, ]

# data pre-processing
# logistic regression takes in factor type variables
train$diabetes <- as.factor(train$diabetes)
```

\subsection{Modeling}
```{r}
log_mod <- glm(diabetes ~., data = train, family = "binomial")
summary(log_mod)
```

\subsection{Prediction}
```{r}
pred <- predict(log_mod, test, type = "response")

# confusion matrix to check prediction accuracy
table(Actual_value = test$diabetes, Predicted_value = pred > 0.5)
```

\section{Decision Tree}
\begin{enumerate}
  \item Decision tree is a tree shape algorithm that is used to determine a course of actions, with each branch on the tree representing a possible decision
  \item Decision tree can be used to solve classification problems
  \item Decision tree can also be used to solve continuous predictions such as regression
\end{enumerate}
Entropy describes the messiness of the problem being classified. The messier the problem is, the larger the entropy. In decision tree problems, we can use the change in entropy to determine what the decision node is. \textbf{The optimum decision node is where the information gain is the largest (i.e. reduces the most entropy)}.

The entropy of decision problem can be calculated as:
$$-\Sigma_{x=1}^{i}p(value_x)log_2(p(value_x))$$
Where value is the proportion of occurrence of one group $value_x = \frac{counts\ \ in\ \ one\ \ group}{total\ \ counts}$

Whichever category can reduce the entropy the greatest will be the node for classification.

\subsection{Data}
```{r}
path <- 'https://raw.githubusercontent.com/guru99-edu/R-Programming/master/titanic_data.csv'
titanic <-read.csv(path)

# 
```

\subsection{Modeling}
```{r}

```

\subsection{Prediction}
