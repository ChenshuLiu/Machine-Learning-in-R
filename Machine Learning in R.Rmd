---
title: "Machine Learning in R"
author: "Chenshu Liu"
date: "April 2022"
output:
  pdf_document:
    toc: true
    toc_depth: 2
  html_document: default
---

\newpage
\section{Packages}
```{r, message = F}
# for data splitting into training and testing (general data manipulation)
library(caTools)

# dataset for logistic regression
# install.packages("mlbench")
library(mlbench)

# Decision tree
# install.packages("FSelector")
# install.packages("caret", dependencies = T)
# install.packages("rpart.plot")
# install.packages("data.tree")
# install.packages("rJava")

# for constructing decision tree
system("java -version")
library(FSelector)
library(rpart)
# for test train set split
library(caret)
library(dplyr)
# plotting the decision tree
library(rpart.plot)
library(data.tree)

# Random forest
# install.packages("randomForest")
library(randomForest)

# Support Vector Machine
library(e1071)
```

\newpage
\section{Linear Regression}
Linear regression is a way to find the best fit linear expression that can show the observed trend(s). The parameters of the best fit line $y = mx + c$ can be calculated by:
$$m = \frac{(n\times \Sigma(x\times y)) - (\Sigma(x)\times \Sigma(y))}{(n\times \Sigma(x^2)) - (\Sigma(x)^2)}$$
$$c = \frac{(\Sigma(y)\times \Sigma(x^2)) - (\Sigma(x)\times \Sigma(x\times y))}{(n\times \Sigma(x^2)) - (\Sigma(x)^2)}$$

\subsection{Data}
```{r, message = FALSE}
sales <- read.csv("/Users/chenshu/Documents/Programming/R/Machine Learning in R/datasets/revenue.csv")

# split into training and testing data
set.seed(2)
# SplitRatio means the percentage of data for training
split <- caTools::sample.split(sales$Profit, SplitRatio = 0.7)
train <- sales[split, ]
test <- sales[!split, ]
```

\subsection{Modeling}
```{r}
Model <- lm(Profit ~., data = train)
summary(Model)
```

\subsection{Predict}
```{r}
pred <- predict(Model, test)

# comparing predicted vs. actual values
plot(test$Profit, type = 'l', lty = 1.8, col = "red")
lines(pred, type = 'l', lty = 1.8, col = "blue")

# determining prediction accuracy
rmse <- sqrt(mean(pred - test$Profit)^2)
rmse
```

\newpage
\section{Logistic Regression}
Logistic regression is a \textbf{classification algorithm}, not a linear prediction algorithm. Different from linear regression, which is usually used to determine the magnitude of the effect, logistic regression is used to predict binary outcome.
\subsection{Data}
```{r}
data(PimaIndiansDiabetes)
log_df <- PimaIndiansDiabetes
head(log_df)

# splitting data
split <- caTools::sample.split(log_df$diabetes, SplitRatio = 0.7)
train <- log_df[split, ]
test <- log_df[!split, ]

# data pre-processing
# logistic regression takes in factor type variables
train$diabetes <- as.factor(train$diabetes)
```

\subsection{Modeling}
```{r}
log_mod <- glm(diabetes ~., data = train, family = "binomial")
summary(log_mod)
```

\subsection{Prediction}
```{r}
pred <- predict(log_mod, test, type = "response")

# confusion matrix to check prediction accuracy
table(Actual_value = test$diabetes, Predicted_value = pred > 0.5)
```

\newpage
\section{Decision Tree}
\begin{enumerate}
  \item Decision tree is a tree shape algorithm that is used to determine a course of actions, with each branch on the tree representing a possible decision
  \item Decision tree can be used to solve classification problems
  \item Decision tree can also be used to solve continuous predictions such as regression
\end{enumerate}
Entropy describes the messiness of the problem being classified. The messier the problem is, the larger the entropy. In decision tree problems, we can use the change in entropy to determine what the decision node is. \textbf{The optimum decision node is where the information gain is the largest (i.e. reduces the most entropy)}.

The entropy of decision problem can be calculated as:
$$-\Sigma_{x=1}^{i}p(value_x)log_2(p(value_x))$$
Where value is the proportion of occurrence of one group $value_x = \frac{counts\ \ in\ \ one\ \ group}{total\ \ counts}$

Whichever category can reduce the entropy the greatest will be the node for classification.

\subsection{Data}
```{r}
path <- 'https://raw.githubusercontent.com/guru99-edu/R-Programming/master/titanic_data.csv'
titanic <-read.csv(path)

# data cleaning
titanic <- select(titanic, survived, pclass, sex, age)
titanic <- mutate(titanic, survived = factor(survived), age = as.numeric(age))

# data splitting
set.seed(123)
sample = sample.split(titanic$survived, SplitRatio = 0.7)
train <- titanic[sample, ]
test <- titanic[!sample, ]
```

\subsection{Modeling}
```{r}
tree <- rpart(survived ~., data = train)
```

\subsection{Prediction}
```{r}
tree.survived.pred <- predict(tree, test, type = "class")

# evaluate model accuracy
confusionMatrix(tree.survived.pred, test$survived)

# visualize decision tree
prp(tree)
```

\newpage
\section{Random Forest}
Random forest works by building multiple decision trees, which can be applied for classification and regression purposes. Random forest is an ensemble of decision trees that are classified based on different standards, and then which ever classification appears the most in the result (i.e. highest frequency), will be defined as the final decision result.

helpful link: https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/ 

\subsection{Data}
```{r}
winequality <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep = ";")

# data manipulation
winequality <- mutate(winequality, quality = as.factor(quality))

# data split
sample <- sample.split(winequality$quality, SplitRatio = 0.7)
train <- winequality[sample, ]
test <- winequality[!sample, ]
```

\subsection{Modeling}
Arguments in the randomForest function:
\begin{itemize}
  \item mtry: the number of variables randomly sampled as candidate for each decision tree. the recommended mtry value is $mtry = \lfloor\sqrt{ncol(x)}\rfloor$
  \item ntree: in order to be a tie breaker, we shall set the ntree argument to an odd number. the argument specifies the number of decision trees that will be constructed
  \item importance: TRUE or FALSE argument, defines whether or not to check the correlations between variables
\end{itemize}
```{r}
rf <- randomForest(quality ~., data = train, 
                   mtry = floor(sqrt(ncol(winequality))),
                   ntree = 2001, importance = TRUE)
rf
```

\subsection{Prediction}
```{r}
result <- data.frame(test$quality, predict(rf, test[, 1:11], type = "response"))
plot(result)

# accuracy
sum(result[, 1] == result[, 2])/nrow(result)
```

\section{Support Vector Machine}
Support vector machine is a binary classification technique. SVM may be a better classification method than logistic regression when the number of variables is large. SVM is achieved by finding the best separating boundary line by computing the maximum distance margin ($D^- + D^+$) from equidistant support vectors (i.e. finding points from both groups that are close to each other as support vectors to support the algorithm). The resulting dividing line is called a hyperplane

SVM kernel function will be used when the current dimension does not support linearly separable data points. The kernel function will increase the dimension to where there is possible linear separation

\subsection{Data}
```{r}
data(PimaIndiansDiabetes)
df <- PimaIndiansDiabetes
# for the sake of demonstration, we will use 2 variables (i.e. 2D space)
df <- df[, c("glucose", "insulin", "diabetes")]

# splitting data
split <- caTools::sample.split(df$diabetes, SplitRatio = 0.7)
train <- df[split, ]
test <- df[!split, ]
head(train)

# data pre-processing
train$diabetes <- as.character(train$diabetes)
train$diabetes <- ifelse(train$diabetes == "pos", +1, -1)
head(train)
```

\subsection{Modeling}
```{r}
# finding best linear boundary through SVM
svm.model <- svm(diabetes ~., data = train,
                 type = "C-classification", kernel = "linear",
                 scale = FALSE)
summary(svm.model)
```

\subsection{Prediction}
```{r}
pred <- predict(svm.model, test)
test$diabetes <- ifelse(test$diabetes == "pos", +1, -1)
table(Actual_value = test$diabetes, Predicted_value = pred)
```

\section{Clustering}
Hierarchical clustering stops when the grouping result in just one group. Agglomerative clustering is a botton-up approach, and divisive approach is a top-down approach. Hierarchical clustering works through finding the closest (pair with the least distance separation) pair of points and grouping them together (forming the dendogram).

There are different ways of determining the distance between points:
\begin{enumerate}
  \item Euclidean distance measure: $d = \sqrt{\Sigma_{i=1}^n(q_i-p_i)^2}$
  \item Squared euclidean distance measure: $d = \Sigma_{i=1}^n(q_i-p_i)^2$
  \item Manhattan distance measure (sum of the horizontal and vertical components between the two points): $d = \Sigma_{i=1}^n |q_x-p_x| + |q_y - p_y|$ a a a
  \item Cosine distance measure
\end{enumerate}

